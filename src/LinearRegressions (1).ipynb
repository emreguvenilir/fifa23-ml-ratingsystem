{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdd0187f-5fec-4155-a750-464ba38b9dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2871835/3486642863.py:13: DtypeWarning: Columns (108) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  fifa_players = pd.read_csv('male_players.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from thefuzz import process\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Load the first CSV file into a DataFrame\n",
    "fifa_players = pd.read_csv('male_players.csv')\n",
    "\n",
    "combined_player_stats = pd.read_csv(\"filtered_combined_player_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d996d020-8ae1-4074-8718-1591ccaf36e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2880\n"
     ]
    }
   ],
   "source": [
    "#league_id \n",
    "#LaLiga - 53\n",
    "#Prem - 13\n",
    "#Bundesliga - 19\n",
    "#Serie A - 31\n",
    "#Ligue 1 - 16\n",
    "\n",
    "filtered_fifa_players = fifa_players[fifa_players['league_id'].isin([53, 13, 19, 31, 16])]\n",
    "\n",
    "fifa23_players = filtered_fifa_players[filtered_fifa_players['fifa_version'].isin([23])]\n",
    "fifa23_players.head()\n",
    "print(fifa23_players.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a9f19f9-10cf-426d-82dd-c421aa77ebce",
   "metadata": {},
   "outputs": [],
   "source": [
    "known_names = fifa23_players['long_name']\n",
    "\n",
    "# Add a new column called 'rating' to the player_stats DataFrame\n",
    "combined_player_stats['rating'] = None\n",
    "\n",
    "# Loop through each corrupted name in the player_stats DataFrame\n",
    "for idx, name in enumerate(combined_player_stats['Player']):\n",
    "    # Find the best match from the list of known names\n",
    "    best_match = process.extractOne(name, known_names)\n",
    "    if best_match is not None:\n",
    "        matched_name = best_match[0]\n",
    "        rating = fifa23_players.loc[fifa23_players['long_name'] == matched_name, 'overall'].iloc[0]\n",
    "        combined_player_stats.at[idx, 'rating'] = rating\n",
    "        # Additional checks\n",
    "        squad = combined_player_stats.at[idx, 'Squad']\n",
    "        born_year = combined_player_stats.at[idx, 'Born']\n",
    "        # Find the matched player's info\n",
    "        matched_player = fifa23_players[fifa23_players['long_name'] == matched_name]\n",
    "        if not matched_player.empty:\n",
    "            matched_squad = matched_player['club_name'].iloc[0]\n",
    "            matched_born_year = int(matched_player['dob'].iloc[0].split('-')[0])\n",
    "            if born_year == matched_born_year:\n",
    "                combined_player_stats.at[idx, 'rating'] = rating/99\n",
    "                #print(best_match)\n",
    "                #print(f\"Matched name: {name}, Best match: {matched_name}, Rating: {rating}\")\n",
    "                #print(f\"Squad and birth year matched for {name}\")\n",
    "            else:\n",
    "                combined_player_stats.drop(idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be4dc24a-7f43-4eec-a9ef-413a858548df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with 0\n",
    "combined_player_stats.fillna(0, inplace=True)\n",
    "#shuffle the players\n",
    "combined_player_stats = combined_player_stats.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Split positions into a list\n",
    "combined_player_stats['Positions_list'] = combined_player_stats['Pos'].str.split(',')\n",
    "\n",
    "# Get unique positions\n",
    "unique_positions = set(position for sublist in combined_player_stats['Positions_list'] for position in sublist)\n",
    "\n",
    "# Create binary indicator variables for each position\n",
    "for position in unique_positions:\n",
    "    combined_player_stats[position] = combined_player_stats['Positions_list'].apply(lambda x: 1 if position in x else 0)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "combined_player_stats.drop(['Positions_list'], axis=1, inplace=True)\n",
    "\n",
    "combined_player_stats.head()\n",
    "\n",
    "# Reorder the columns to make 'rating' the last column\n",
    "columns_ordered = [col for col in combined_player_stats.columns if col != 'rating'] + ['rating']\n",
    "\n",
    "# Reassign the DataFrame with the reordered columns\n",
    "combined_player_stats = combined_player_stats[columns_ordered]\n",
    "\n",
    "# Display the updated DataFrame\n",
    "#combined_player_stats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b47efe8-5724-433a-8e64-0a108ed99784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.030769575384558923\n",
      "Mean Squared Error (MSE): 0.0016752951101287707\n",
      "Root Mean Squared Error (RMSE): 0.0409303690446198\n",
      "R-squared (R2): 0.6738410573952047\n"
     ]
    }
   ],
   "source": [
    "#Linear regression with categorical variables\n",
    "\n",
    "combined_player_stats['Squad'] = combined_player_stats['Squad'].astype(str)\n",
    "combined_player_stats['Comp'] = combined_player_stats['Comp'].astype(str)\n",
    "combined_player_stats['Nation'] = combined_player_stats['Nation'].astype(str)\n",
    "#combined_player_stats['Pos'] = combined_player_stats['Pos'].astype(str)\n",
    "\n",
    "# Splitting the data into input (X) and output (Y) variables\n",
    "X = combined_player_stats.iloc[1:, :-1].drop(columns=['Player','Pos'])\n",
    "Y = combined_player_stats.iloc[1:, -1]   # Only the last column, excluding first row\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_cols = ['Squad', 'Comp', 'Nation']\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    [('onehotencoder', OneHotEncoder(handle_unknown='ignore'), categorical_cols)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Define the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', column_transformer),\n",
    "    ('regressor', model)\n",
    "])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ca9562b-d248-45fe-ab79-b8d3a03c170d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.03341796291947891\n",
      "Mean Squared Error (MSE): 0.0019000173866472244\n",
      "Root Mean Squared Error (RMSE): 0.04358918887347211\n",
      "R-squared (R2): 0.6300904491317044\n"
     ]
    }
   ],
   "source": [
    "#Linear regression without categorical vars\n",
    "\n",
    "X = combined_player_stats.iloc[1:, 5:-1].values\n",
    "Y = combined_player_stats.iloc[1:, -1]   # Only the last column, excluding first row\n",
    "\n",
    "# Define the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Predictions\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2):\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1146b20-1ffe-4dd8-906b-14e7242286c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression on just goalkeepers since their metrics are so different to the rest\n",
    "#of the players, smaller dataset though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da232eb1-8219-41a2-90c4-b7e0461f3ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df: 296\n"
     ]
    }
   ],
   "source": [
    "#creating GK dataframe \n",
    "# Create a new DataFrame containing only players with position 'GK'\n",
    "gk_players_df = combined_player_stats[combined_player_stats['Pos'] == 'GK'].copy()\n",
    "\n",
    "# Columns to keep: columns ending with \"_GK\" and additional columns\n",
    "cols_to_keep = [col for col in gk_players_df.columns if col.endswith('_GK')] + ['Age', 'Born', 'MP', 'Minutes_Played', 'Mn_per_MP', 'Mins_Per_90', 'Starts', 'PPM_Team.Success', 'onG_Team.Success', 'onGA_Team.Success', 'CrdY', 'CrdR', 'PrgP','rating']\n",
    "\n",
    "# Create a new DataFrame containing the necessary columns for the GK model\n",
    "gk_model_df = gk_players_df[cols_to_keep].copy()\n",
    "\n",
    "print(\"Number of rows in df:\", gk_model_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11c11492-49d4-4174-a032-1ead8373af61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best hyperparameters: {'alpha': 100}\n",
      "Mean Absolute Error (MAE): 0.0414436848084614\n",
      "Mean Squared Error (MSE): 0.002845500647700676\n",
      "Root Mean Squared Error (RMSE): 0.05334323431983363\n",
      "R-squared (R2): 0.6128601187439746\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#Testing goalkeepers on their own\n",
    "# Remove the first row from the DataFrame to exclude headers or invalid data\n",
    "gk_model_df = gk_model_df.iloc[1:]\n",
    "\n",
    "# Split the data into features (X) and target variable (Y)\n",
    "X = gk_model_df.iloc[:, 5:-1].values  # Excluding 'rating' and other non-feature columns\n",
    "Y = gk_model_df['rating'].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the Ridge regression model\n",
    "ridge = Ridge()\n",
    "\n",
    "# Define the hyperparameters grid\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1, 10, 100] \n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error', verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to find the best hyperparameters\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Predictions\n",
    "Y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Compute evaluation metrics\n",
    "mae = mean_absolute_error(Y_test, Y_pred)\n",
    "mse = mean_squared_error(Y_test, Y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(Y_test, Y_pred)\n",
    "\n",
    "# Print best hyperparameters and evaluation metrics\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)\n",
    "print(\"R-squared (R2):\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a28e0bc-03d4-49c6-919f-73c5d4339954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.04273171806218458\n",
      "Mean Squared Error (MSE): 0.002945195665476465\n",
      "Root Mean Squared Error (RMSE): 0.05340388626019442\n",
      "Mean R-squared (R2): 0.5800373073998699\n"
     ]
    }
   ],
   "source": [
    "# Remove the first row from the DataFrame to exclude headers or invalid data\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "gk_model_df = gk_model_df.iloc[1:]\n",
    "\n",
    "# Split the data into features (X) and target variable (Y)\n",
    "X = gk_model_df.iloc[:, 5:-1].values  # Excluding 'rating' and other categorical columns\n",
    "Y = gk_model_df['rating'].values\n",
    "\n",
    "# Define number of splits for k-fold cross-validation\n",
    "n_splits = 10\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics for each fold\n",
    "mae_scores, mse_scores, rmse_scores, r2_scores = [], [], [], []\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    # Split the data into training and testing sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "    \n",
    "    # Define the linear regression model\n",
    "    linear_reg = Ridge(alpha=10)\n",
    "    \n",
    "    # Fit the model\n",
    "    linear_reg.fit(X_train, Y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    Y_pred = linear_reg.predict(X_test)\n",
    "    \n",
    "    # Compute evaluation metrics for this fold\n",
    "    mae = mean_absolute_error(Y_test, Y_pred)\n",
    "    mse = mean_squared_error(Y_test, Y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(Y_test, Y_pred)\n",
    "    \n",
    "    # Append evaluation metrics to the respective lists\n",
    "    mae_scores.append(mae)\n",
    "    mse_scores.append(mse)\n",
    "    rmse_scores.append(rmse)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "# Compute mean evaluation metrics across all folds\n",
    "mean_mae = np.mean(mae_scores)\n",
    "mean_mse = np.mean(mse_scores)\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "mean_r2 = np.mean(r2_scores)\n",
    "\n",
    "# Print mean evaluation metrics\n",
    "print(\"Mean Absolute Error (MAE):\", mean_mae)\n",
    "print(\"Mean Squared Error (MSE):\", mean_mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", mean_rmse)\n",
    "print(\"Mean R-squared (R2):\", mean_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c07783a-6d17-47ae-bfb8-a7b6005f915d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
